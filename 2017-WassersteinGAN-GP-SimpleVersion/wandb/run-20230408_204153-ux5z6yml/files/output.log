
Epoch: 1:   0%|                                                           | 0/50000 [00:11<?, ?it/s]
Traceback (most recent call last):
  File "C:\Users\Administrator\OneDrive\Documents\Deep learning\Super Resolution\Models\2017-WassersteinGAN-SimpleVersion\main.py", line 130, in <module>
    loss_D.backward()
  File "D:\Anaconda\lib\site-packages\torch\_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "D:\Anaconda\lib\site-packages\torch\autograd\__init__.py", line 197, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.